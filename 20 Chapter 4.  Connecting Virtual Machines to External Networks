### 20 Chapter 4.  Connecting Virtual Machines to External Networks 

The process to connect a VM to an externat network with additional network interfaces is named: "Multihoming".

This is achieved throught the Operators: NMState & Multus.
In K8s pods communication is direct and thre is no need of NAT.
Pods and system processes can communicate when they share on the same node.
Containers in the same pod can communicate using the "lo" network interface and with the outside environment,
using different ports.

Multus CNI Plugin calls other CNI plugins for advanced networking functions: this is called "Chaining".
Although you can add another network interface to the pod, the eth0 must always exists. 
You cannot add net1 network interface and remove the eth0 network interface.

Attaching a second network can be very important to isolate critical traffic for some kind of application, 
that needs more security and higher performances, to implement some kind of authentication and encryption.

In OpenShift the ClusterNetworkAddon operator is the one that manage additional network configurations, based
on the Multus CNI plug-in.
The Multus CNI plug-in is implemented through the "NetworkAttacchementDefinition" custom resource.

A "NetworkAttacchementDefinition" is namespaced resources and it resides where the pods need to have an 
additional external network interface. 

A "NetworkAttachementDefinition" exposes the namespace for pods to an external network. 

A "NetworkAttachementDefinition" requires you to specify what CNI Network Plugin you want to use for pods and vmis.

Multus in sé non crea mai direttamente interfacce di rete: è un “meta‑plug‑in” CNI che ti permette di collegare (chain) 
altri plug‑in CNI veri e propri, ognuno dei quali implementa un diverso tipo di collegamento di rete.

Vedi quindi l’elenco dei plug‑in CNI che Red Hat OpenShift mette a disposizione da “agganciare” tramite Multus:

#bridge
Utilizza un bridge Linux per creare una rete “virtuale” interna al nodo, in modo che più pod possano parlarsi 
tra loro (e con l’host) tramite un interfaccia bridge.

#host‑device
Fa passare direttamente al pod un device fisico di rete presente sull’host (PCI passthrough) 
senza virtualizzazione: utile se vuoi che il pod parli “nativo” con la scheda di rete.

#ipvlan
Sfrutta la modalità IPVLAN del kernel: tutti i pod condividono il MAC dell’interfaccia fisica, 
ma ottengono IP distinti. Leggero, a basse latenze, ma non raggiungibile da macchine esterne alla sottorete.

#macvlan
Simile a ipvlan, ma crea più interfacce virtuali, ognuna con MAC diverso. I pod possono comunicare fra loro 
e con altri host della rete fisica come se fossero macchine autonome.

#SR‑IOV
Se hai hardware di rete SR‑IOV‑capable, puoi “strappargli” fino a decine di Virtual Functions (VF) e assegnarle 
ai pod per avere prestazioni near‑bare‑metal (meno overhead di virtualizzazione).

Quindi, riassumendo:

1 - Multus = orchestratore/meta‑plug‑in, ti permette di attaccare più reti ai tuoi pod

2 - bridge/host‑device/ipvlan/macvlan/SR‑IOV = i plug‑in CNI “backend” che effettivamente configurano le interfacce 
sulla base della tecnologia scelta.


### Ma questi bridge, host-device, ipvlan, macvlan, SR‑IOV sono plugin come Calico e Flannel?

Sì, anche bridge, host‑device, ipvlan, macvlan e SR‑IOV sono dei veri e propri plug‑in CNI (esattamente come Calico o Flannel), 
nel senso che ciascuno di essi ha il suo binario/esecutore che rispetta la specifica CNI e crea/configura interfacce di rete nei pod.


La differenza principale sta nel ruolo che svolgono:

#Calico, Flannel, Weave, Cilium…
Sono spesso indicati come “network fabric” o “network providers”: gestiscono l’assegnazione degli IP, l’instradamento (BGP o VXLAN), 
policy di rete (NetworkPolicy), e forniscono la rete “primaria” (default) per tutti i pod del cluster.

#bridge, host‑device, ipvlan, macvlan, SR‑IOV
Sono plug‑in “secondari” o “di attacco” (attachment) usati tipicamente con Multus per creare multiple interfacce nei pod, ognuna 
su reti diverse. Non hanno un controller BGP né un sistema avanzato di policy, ma si limitano a “collegare” il namespace del pod 
a un bridge Linux, a una VF SR‑IOV, a un macvlan/ipvlan, ecc.

In pratica:

Senza Multus
– Calico/Flannel/… attaccano un’unica rete CNI a ogni pod.

Con Multus
– Multus usa uno “primary” plugin (ad esempio Calico) per la first network,
– e poi “chaina” uno o più di questi (bridge, macvlan, SR‑IOV…) per aggiungere seconde (o terze, …) interfacce ai pod.

Quindi, sì: sono plug‑in CNI al pari di Calico e Flannel, ma sono nati per casi d’uso “multihomed” 
(più schede di rete in un singolo pod) piuttosto che per fornire l’intera connettività di cluster.

### ALL THESE NETWORK COMPONENTS NEEDS TO BE CONFIGURED BEFORE CREATING A NETWORKATTACHEMENTDEFINITION






###############################################################################
#
#


Il problema da risolvere
Hai un pod (o una VM) in Kubernetes e vuoi che, oltre alla sua solita interfaccia di rete “principale”, ne abbia un’altra collegata a un bridge speciale (per esempio una rete di management o di storage separata).

Cosa sono i pezzi in gioco
La “ricetta” del bridge
– In KubeVirt (o in un altro CNI avanzato) hai già definito qualcosa (una CRD) che sa come costruire un bridge Linux sul nodo:

si chiama “bridge‑dev”

sa su quale interfaccia fisica appoggiarsi, se applicare VLAN, MAC‑spoof‑check, ecc.

Il NetworkAttachmentDefinition (NAD)
– È il pezzetto Kubernetes che contiene il JSON del plugin (cnv‑bridge, macvlan, ecc.), ossia la configurazione “grezza” di CNI:

che tipo di plugin usare

come chiamare il bridge

eventuali opzioni (VLAN, macspoofchk…)

Il ruolo dell’annotation
L’annotation k8s.v1.cni.cncf.io/resourceName: bridge.network.kubevirt.io/bridge-dev serve da collegamento:

Senza annotation: il NAD ha una configurazione, ma non “sa” a quale definizione esterna quella configurazione si riferisca.

Con l’annotation: dici a Multus/KubeVirt:

“Quando mi chiedi di applicare questo NAD, vai a leggere la CRD di nome bridge-dev nel gruppo bridge.network.kubevirt.io e usa i suoi dettagli per creare il bridge sul nodo.”

Cosa succede “in pratica” quando parte il pod
Il pod chiede a Multus di attaccarsi alla rete “bridge-dev”.

Multus legge il NAD bridge-dev e trova l’annotation resourceName.

Va a prendere la definizione esterna bridge.network.kubevirt.io/bridge-dev, recupera tutti i parametri (nome del bridge, VLAN, macspoofchk, ecc.).

Invoca il plugin CNI (cnv-bridge) con quella configurazione:

crea l’interfaccia nel namespace del pod

la collega al bridge Linux giusto sul nodo

applica le regole di sicurezza (MAC spoofing check, VLAN, …)

In una frase:

L’annotation serve a legare il tuo oggetto Kubernetes (NAD) alla definizione di rete esterna già creata (CRD), così Multus sa esattamente quale bridge o quale risorsa “vera” deve configurare quando partono i pod.








quindi quella annotation dice a k8s su quale crd appoggiarsi per creare la nad. quella crd su cui è appoggiarsi è lo specifico cni. 


ChatGPT ha detto:

Ha riflettuto per 4 secondi
Esatto, hai centrato il punto:

L’annotation k8s.v1.cni.cncf.io/resourceName nel tuo NAD indica esattamente quale CRD esterna (cioè quale risorsa custom nel cluster) Multus deve “leggere” per ottenere il tipo di CNI e i parametri da applicare.

Quella CRD esterna è fornita dal plugin CNI specifico (es. il CRD bridge.network.kubevirt.io/bridge-dev per il plugin cnv‑bridge, oppure sriovnetwork.openshift.io/mia-rete-sriov per il plugin SR‑IOV, e così via).

In pratica:

Definisci la CRD del plugin (che sa come creare il bridge, la VF SR‑IOV, il macvlan, ecc.).

Crei un NetworkAttachmentDefinition standard, e nell’annotation resourceName ci metti “<apiGroup>/<nomeCRD>”.

Quando parte il pod, Multus va a quel CRD, ne legge il “tipo di CNI” e le opzioni, e configura l’interfaccia di conseguenza.

Così il tuo NAD diventa “vivo”: non è solo un JSON generico, ma sa qual è il plugin esatto e la configurazione precisa da usare grazie a quella sola riga di annotation.

 oltre a collegare il tuo NAD alla CRD “bridge.network.kubevirt.io/bridge‑dev”, quella annotation fa scattare anche il meccanismo di node selection per le VM (e i pod) che la usano.

In breve:

ResourceName → dice a Multus/KubeVirt quale CRD usare per configurare davvero il bridge.

Side‑effect di KubeVirt‑Network‑Operator → in fase di startup l’operator rileva su quali nodi è presente il bridge bridge-dev (o la VF SR‑IOV, o il macvlan, ecc.) e applica una label a quegli stessi nodi (es. network.bridge-dev=present).

Scheduling → quando crei una VM (o un pod) con quella NAD, il scheduler KubeVirt legge l’annotation, capisce quale label cercare e schedula la VM solo sui nodi etichettati.

Senza l’annotation:

Il NAD rimane “solo” un JSON che punti a un bridge, ma non ci sarà alcuna label sui nodi

Sarà tua cura (via nodeSelector/affinity) evitare di mandare il pod/VM su macchine che non hanno il bridge

Con l’annotation:

KubeVirt‑Network‑Operator gestisce in automatico il labeling dei nodi

Il tuo workload arriverà solo dove i prerequisiti di rete (il bridge‑dev) sono davvero presenti

Così non rischi di finire su un nodo che non ha il bridge e vederti il CNI fallire.
